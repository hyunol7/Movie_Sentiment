"""
ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
ìµœì†Œí•œì˜ ëª¨ë¸ë§Œ í•™ìŠµí•˜ì—¬ ë¹ ë¥´ê²Œ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import os
import sys
import subprocess
import time
from datetime import datetime

def run_command(command, description, timeout=None):
    """ëª…ë ¹ì–´ ì‹¤í–‰ í•¨ìˆ˜"""
    print(f"\nğŸš€ {description}")
    print("-" * 40)
    
    start_time = time.time()
    
    try:
        result = subprocess.run(
            command, 
            shell=True, 
            check=True, 
            capture_output=True, 
            text=True,
            timeout=timeout
        )
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        print(f"âœ… ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
        return True
        
    except subprocess.TimeoutExpired:
        print(f"â° ì‹œê°„ ì´ˆê³¼ ({timeout}ì´ˆ)")
        return False
    except subprocess.CalledProcessError as e:
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        print(f"âŒ ì‹¤íŒ¨! (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
        if e.stderr:
            print(f"ì˜¤ë¥˜: {e.stderr}")
        return False

def check_quick_requirements():
    """ë¹ ë¥¸ ì‹œì‘ì„ ìœ„í•œ ìµœì†Œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    print("ğŸ” ë¹ ë¥¸ ì‹œì‘ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    required_files = ["IMDB Dataset.csv", "netflix_titles.csv"]
    missing_files = []
    
    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)
    
    if missing_files:
        print("âŒ ë‹¤ìŒ íŒŒì¼ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤:")
        for file in missing_files:
            print(f"  - {file}")
        return False
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("data", exist_ok=True)
    os.makedirs("data/models", exist_ok=True)
    os.makedirs("static", exist_ok=True)
    
    print("âœ… ìš”êµ¬ì‚¬í•­ í™•ì¸ ì™„ë£Œ!")
    return True

def create_quick_preprocessor():
    """ë¹ ë¥¸ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    quick_preprocessor = """
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import joblib

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

print("ë¹ ë¥¸ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

# IMDB ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§Œ)
df = pd.read_csv("IMDB Dataset.csv")
print(f"ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ")

# ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ìƒ˜í”Œë§Œ ì‚¬ìš©
df_sample = df.sample(n=10000, random_state=42)
print(f"ìƒ˜í”Œ ë°ì´í„°: {len(df_sample)}ê°œ")

# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì •ì œ
def simple_clean(text):
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-zA-Z\\s]', '', text)
    return text.lower().strip()

df_sample['cleaned_review'] = df_sample['review'].apply(simple_clean)
df_sample['sentiment_label'] = df_sample['sentiment'].map({'positive': 1, 'negative': 0})

# ë°ì´í„° ë¶„í• 
X = df_sample['cleaned_review']
y = df_sample['sentiment_label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# TF-IDF (ê°„ë‹¨ ë²„ì „)
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# ì €ì¥
joblib.dump(X_train_tfidf, "data/X_train_tfidf_quick.pkl")
joblib.dump(X_test_tfidf, "data/X_test_tfidf_quick.pkl")
joblib.dump(y_train, "data/y_train_quick.pkl")
joblib.dump(y_test, "data/y_test_quick.pkl")
joblib.dump(tfidf, "data/tfidf_vectorizer_quick.pkl")

# Netflix ë°ì´í„° (ê°„ë‹¨ ì²˜ë¦¬)
netflix_df = pd.read_csv("netflix_titles.csv")
movies_df = netflix_df[netflix_df['type'] == 'Movie'].copy()
movies_df['description'] = movies_df['description'].fillna('')
movies_df.to_csv("data/netflix_movies_quick.csv", index=False)

print("âœ… ë¹ ë¥¸ ì „ì²˜ë¦¬ ì™„ë£Œ!")
"""
    
    with open("quick_preprocess.py", "w", encoding="utf-8") as f:
        f.write(quick_preprocessor)

def create_quick_model():
    """ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    quick_model = """
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

print("ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

# ë°ì´í„° ë¡œë“œ
X_train = joblib.load("data/X_train_tfidf_quick.pkl")
X_test = joblib.load("data/X_test_tfidf_quick.pkl")
y_train = joblib.load("data/y_train_quick.pkl")
y_test = joblib.load("data/y_test_quick.pkl")

# ê°„ë‹¨í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ëª¨ë¸ ì •í™•ë„: {accuracy:.4f}")
print("\\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))

# ëª¨ë¸ ì €ì¥
joblib.dump(model, "data/models/best_model.pkl")
print("âœ… ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
"""
    
    with open("quick_model.py", "w", encoding="utf-8") as f:
        f.write(quick_model)

def create_quick_recommendation():
    """ë¹ ë¥¸ ì¶”ì²œ ì‹œìŠ¤í…œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    quick_rec = """
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import joblib

print("ë¹ ë¥¸ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œì‘...")

# ì˜í™” ë°ì´í„° ë¡œë“œ
movies_df = pd.read_csv("data/netflix_movies_quick.csv")
print(f"ì˜í™” ìˆ˜: {len(movies_df)}")

# ê°„ë‹¨í•œ ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ
tfidf = TfidfVectorizer(max_features=1000, stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies_df['description'])

# ìœ ì‚¬ë„ ê³„ì‚° (ìƒ˜í”Œë§Œ)
sample_size = min(1000, len(movies_df))
sample_indices = np.random.choice(len(movies_df), sample_size, replace=False)
sample_matrix = tfidf_matrix[sample_indices]
similarity_matrix = cosine_similarity(sample_matrix)

# ì €ì¥
np.save("data/models/content_similarity_matrix.npy", similarity_matrix)
movies_df.iloc[sample_indices].to_csv("data/models/processed_movies.csv", index=False)

print("âœ… ë¹ ë¥¸ ì¶”ì²œ ì‹œìŠ¤í…œ ì™„ë£Œ!")
"""
    
    with open("quick_recommendation.py", "w", encoding="utf-8") as f:
        f.write(quick_rec)

def create_quick_streamlit():
    """ë¹ ë¥¸ Streamlit ì•± ìƒì„±"""
    quick_app = """
import streamlit as st
import pandas as pd
import joblib
import re

st.set_page_config(page_title="ì˜í™” ê°ì„± ë¶„ì„ (ë¹ ë¥¸ ë²„ì „)", page_icon="ğŸ¬")

st.title("ğŸ¬ ì˜í™” ê°ì„± ë¶„ì„ (ë¹ ë¥¸ ë²„ì „)")
st.markdown("ê°„ë‹¨í•œ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„ì„ ì²´í—˜í•´ë³´ì„¸ìš”!")

# ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_model():
    try:
        model = joblib.load("data/models/best_model.pkl")
        vectorizer = joblib.load("data/tfidf_vectorizer_quick.pkl")
        return model, vectorizer
    except:
        return None, None

model, vectorizer = load_model()

if model is None:
    st.error("ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. quick_start.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
else:
    # ê°ì„± ë¶„ì„
    st.subheader("ğŸ’¬ ë¦¬ë·° ê°ì„± ë¶„ì„")
    
    user_input = st.text_area("ì˜í™” ë¦¬ë·°ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", height=100)
    
    if st.button("ë¶„ì„í•˜ê¸°") and user_input:
        # ê°„ë‹¨í•œ ì „ì²˜ë¦¬
        cleaned = re.sub(r'<.*?>', '', user_input)
        cleaned = re.sub(r'[^a-zA-Z\\s]', '', cleaned).lower().strip()
        
        # ì˜ˆì¸¡
        X = vectorizer.transform([cleaned])
        prediction = model.predict(X)[0]
        probability = model.predict_proba(X)[0]
        
        # ê²°ê³¼ í‘œì‹œ
        if prediction == 1:
            st.success(f"ğŸ˜Š ê¸ì •ì  (ì‹ ë¢°ë„: {probability[1]:.1%})")
        else:
            st.error(f"ğŸ˜ ë¶€ì •ì  (ì‹ ë¢°ë„: {probability[0]:.1%})")
    
    # ê°„ë‹¨í•œ í†µê³„
    st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
    st.info("ì´ê²ƒì€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ë²„ì „ì…ë‹ˆë‹¤. ì „ì²´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ run_all.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
"""
    
    with open("quick_app.py", "w", encoding="utf-8") as f:
        f.write(quick_app)

def cleanup_quick_files():
    """ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬"""
    quick_files = [
        "quick_preprocess.py",
        "quick_model.py", 
        "quick_recommendation.py",
        "quick_app.py"
    ]
    
    for file in quick_files:
        if os.path.exists(file):
            os.remove(file)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¬ ì˜í™” ê°ì„± ë¶„ì„ & ì¶”ì²œ ì‹œìŠ¤í…œ - ë¹ ë¥¸ ì‹œì‘")
    print("=" * 60)
    print("âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìƒ˜í”Œ ë°ì´í„°ì™€ ê°„ë‹¨í•œ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("ğŸ“Š ì „ì²´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ run_all.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    # ìš”êµ¬ì‚¬í•­ í™•ì¸
    if not check_quick_requirements():
        return
    
    try:
        # ë¹ ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë“¤ ìƒì„±
        print("\nğŸ“ ë¹ ë¥¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")
        create_quick_preprocessor()
        create_quick_model()
        create_quick_recommendation()
        create_quick_streamlit()
        
        # ë‹¨ê³„ë³„ ì‹¤í–‰
        steps = [
            ("python quick_preprocess.py", "ë¹ ë¥¸ ë°ì´í„° ì „ì²˜ë¦¬", 300),
            ("python quick_model.py", "ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ", 120),
            ("python quick_recommendation.py", "ë¹ ë¥¸ ì¶”ì²œ ì‹œìŠ¤í…œ", 60)
        ]
        
        all_success = True
        for command, description, timeout in steps:
            success = run_command(command, description, timeout)
            if not success:
                all_success = False
                break
        
        if all_success:
            print("\n" + "=" * 60)
            print("âœ… ë¹ ë¥¸ ì‹œì‘ ì™„ë£Œ!")
            print("=" * 60)
            
            # ì›¹ ì•± ì‹¤í–‰ ì œì•ˆ
            response = input("\nì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower()
            if response in ['y', 'yes']:
                print("\nğŸŒ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘...")
                print("ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8501 ë¡œ ì ‘ì†í•˜ì„¸ìš”.")
                
                try:
                    subprocess.run("streamlit run quick_app.py", shell=True)
                except KeyboardInterrupt:
                    print("\nâœ… ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
            print("  - ì „ì²´ ê¸°ëŠ¥: python run_all.py")
            print("  - ì›¹ ëŒ€ì‹œë³´ë“œ: streamlit run src/streamlit_app/app.py")
        
        else:
            print("\nâŒ ë¹ ë¥¸ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            print("setup.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ê±°ë‚˜ run_all.pyë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
    
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        cleanup_quick_files()

if __name__ == "__main__":
    main()





